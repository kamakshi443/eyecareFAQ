{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 18: expected 1 fields, saw 2\\nSkipping line 26: expected 1 fields, saw 2\\nSkipping line 56: expected 1 fields, saw 5\\nSkipping line 59: expected 1 fields, saw 2\\nSkipping line 83: expected 1 fields, saw 2\\nSkipping line 87: expected 1 fields, saw 2\\nSkipping line 313: expected 1 fields, saw 2\\nSkipping line 584: expected 1 fields, saw 3\\nSkipping line 649: expected 1 fields, saw 2\\nSkipping line 785: expected 1 fields, saw 2\\nSkipping line 816: expected 1 fields, saw 2\\nSkipping line 838: expected 1 fields, saw 2\\nSkipping line 862: expected 1 fields, saw 2\\nSkipping line 1199: expected 1 fields, saw 3\\nSkipping line 1232: expected 1 fields, saw 2\\nSkipping line 1279: expected 1 fields, saw 2\\nSkipping line 1280: expected 1 fields, saw 2\\nSkipping line 1308: expected 1 fields, saw 2\\nSkipping line 1343: expected 1 fields, saw 2\\nSkipping line 1401: expected 1 fields, saw 2\\nSkipping line 1524: expected 1 fields, saw 3\\nSkipping line 1565: expected 1 fields, saw 2\\nSkipping line 1717: expected 1 fields, saw 2\\nSkipping line 1865: expected 1 fields, saw 2\\nSkipping line 1880: expected 1 fields, saw 2\\nSkipping line 1929: expected 1 fields, saw 2\\nSkipping line 1932: expected 1 fields, saw 2\\nSkipping line 1986: expected 1 fields, saw 3\\nSkipping line 2108: expected 1 fields, saw 2\\nSkipping line 2109: expected 1 fields, saw 2\\nSkipping line 2327: expected 1 fields, saw 2\\nSkipping line 2380: expected 1 fields, saw 3\\nSkipping line 2418: expected 1 fields, saw 2\\nSkipping line 2624: expected 1 fields, saw 2\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABELS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have \"spots\" floating around in my eye. Shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I work on a computer all day. Can this hurt my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm interested in having LASIK done. What info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you tell me what's wrong with my eyes?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My doctor refused to give me my contact lens p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              LABELS\n",
       "0  I have \"spots\" floating around in my eye. Shou...\n",
       "1  I work on a computer all day. Can this hurt my...\n",
       "2  I'm interested in having LASIK done. What info...\n",
       "3         Can you tell me what's wrong with my eyes?\n",
       "4  My doctor refused to give me my contact lens p..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer \n",
    "\n",
    "data = pd.read_csv('eyecareFAQ.csv', error_bad_lines=False)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['LABELS'].dropna(inplace=True)\n",
    "tokens = data['LABELS'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [I, have, ``, spots, '', floating, around, in,...\n",
      "1       [I, work, on, a, computer, all, day, ., Can, t...\n",
      "2       [I, 'm, interested, in, having, LASIK, done, ....\n",
      "3       [Can, you, tell, me, what, 's, wrong, with, my...\n",
      "4       [My, doctor, refused, to, give, me, my, contac...\n",
      "                              ...                        \n",
      "3024    [What, are, symptoms, of, a, rhegmatogenous, r...\n",
      "3025    [Are, there, medications, that, affect, my, ch...\n",
      "3026    [Does, heart, disease, affect, my, chances, of...\n",
      "3027    [Does, cataract, surgery, affect, my, chances,...\n",
      "3028               [What, is, juvenile, retinoschisis, ?]\n",
      "Name: LABELS, Length: 3029, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kam/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: 'U' mode is deprecated\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import nltk,csv,numpy \n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "reader = csv.reader(open('eyecareFAQ.csv', 'rU'), delimiter= \",\",quotechar='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenData = nltk.word_tokenize(str(reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', '_csv.reader', 'object', 'at', '0x7f25c07ecd50', '>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " '``',\n",
       " 'spots',\n",
       " \"''\",\n",
       " 'floating',\n",
       " 'around',\n",
       " 'in',\n",
       " 'my',\n",
       " 'eye',\n",
       " '.',\n",
       " 'Should',\n",
       " 'I',\n",
       " 'be',\n",
       " 'worried',\n",
       " '?']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(str(data[\"LABELS\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " '\"spots\"',\n",
       " 'floating',\n",
       " 'around',\n",
       " 'in',\n",
       " 'my',\n",
       " 'eye.',\n",
       " 'Should',\n",
       " 'I',\n",
       " 'be',\n",
       " 'worried?']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.WhitespaceTokenizer().tokenize(str(data[\"LABELS\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " '\"',\n",
       " 'spots',\n",
       " '\"',\n",
       " 'floating',\n",
       " 'around',\n",
       " 'in',\n",
       " 'my',\n",
       " 'eye',\n",
       " '.',\n",
       " 'Should',\n",
       " 'I',\n",
       " 'be',\n",
       " 'worried',\n",
       " '?']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize using Punctuations\n",
    "nltk.tokenize.WordPunctTokenizer().tokenize(str(data[\"LABELS\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " '``',\n",
       " 'spots',\n",
       " \"''\",\n",
       " 'floating',\n",
       " 'around',\n",
       " 'in',\n",
       " 'my',\n",
       " 'eye.',\n",
       " 'Should',\n",
       " 'I',\n",
       " 'be',\n",
       " 'worried',\n",
       " '?']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.TreebankWordTokenizer().tokenize(str(data[\"LABELS\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have \"spots\" floating around in my eye. Should I be worried?'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Original Sentence\n",
    "data[\"LABELS\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalWords</th>\n",
       "      <th>PorterStemmedWords</th>\n",
       "      <th>SnowballStemmedWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"spots\"</td>\n",
       "      <td>\"spots\"</td>\n",
       "      <td>\"spots\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>floating</td>\n",
       "      <td>float</td>\n",
       "      <td>float</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>around</td>\n",
       "      <td>around</td>\n",
       "      <td>around</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>my</td>\n",
       "      <td>my</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eye.</td>\n",
       "      <td>eye.</td>\n",
       "      <td>eye.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Should</td>\n",
       "      <td>should</td>\n",
       "      <td>should</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>worried?</td>\n",
       "      <td>worried?</td>\n",
       "      <td>worried?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OriginalWords PorterStemmedWords SnowballStemmedWords\n",
       "0              I                  I                    i\n",
       "1           have               have                 have\n",
       "2        \"spots\"            \"spots\"              \"spots\"\n",
       "3       floating              float                float\n",
       "4         around             around               around\n",
       "5             in                 in                   in\n",
       "6             my                 my                   my\n",
       "7           eye.               eye.                 eye.\n",
       "8         Should             should               should\n",
       "9              I                  I                    i\n",
       "10            be                 be                   be\n",
       "11      worried?           worried?             worried?"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STEMMING\n",
    "words  = nltk.tokenize.WhitespaceTokenizer().tokenize(str(data[\"LABELS\"][0]))\n",
    "df = pd.DataFrame()\n",
    "df['OriginalWords'] = pd.Series(words)\n",
    "#porter's stemmer\n",
    "porterStemmedWords = [nltk.stem.PorterStemmer().stem(word) for word in words]\n",
    "df['PorterStemmedWords'] = pd.Series(porterStemmedWords)\n",
    "#SnowBall stemmer\n",
    "snowballStemmedWords = [nltk.stem.SnowballStemmer(\"english\").stem(word) for word in words]\n",
    "df['SnowballStemmedWords'] = pd.Series(snowballStemmedWords)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalWords</th>\n",
       "      <th>WordNetLemmatizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"spots\"</td>\n",
       "      <td>\"spots\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>floating</td>\n",
       "      <td>floating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>around</td>\n",
       "      <td>around</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>my</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eye.</td>\n",
       "      <td>eye.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Should</td>\n",
       "      <td>Should</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>worried?</td>\n",
       "      <td>worried?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OriginalWords WordNetLemmatizer\n",
       "0              I                 I\n",
       "1           have              have\n",
       "2        \"spots\"           \"spots\"\n",
       "3       floating          floating\n",
       "4         around            around\n",
       "5             in                in\n",
       "6             my                my\n",
       "7           eye.              eye.\n",
       "8         Should            Should\n",
       "9              I                 I\n",
       "10            be                be\n",
       "11      worried?          worried?"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LEMMATIZATION\n",
    "words  = nltk.tokenize.WhitespaceTokenizer().tokenize(str(data[\"LABELS\"][0]))\n",
    "df = pd.DataFrame()\n",
    "df['OriginalWords'] = pd.Series(words)\n",
    "#WordNet Lemmatization\n",
    "wordNetLemmatizedWords = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in words]\n",
    "df['WordNetLemmatizer'] = pd.Series(wordNetLemmatizedWords)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
